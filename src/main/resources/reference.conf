hailstorm {

  backend {
    akka {
      loggers = ["akka.event.slf4j.Slf4jLogger"]
      loglevel = "INFO"
      logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

      actor {
        provider = "akka.remote.RemoteActorRefProvider"

        serializers {
          wire = "ch.epfl.labos.hailstorm.serialization.WireSerializer"
        }

        serialization-bindings {
          "java.lang.String" = java
          "ch.epfl.labos.hailstorm.common.HailstormMessage" = wire
          "java.io.Serializable" = none
        }

        enable-additional-serialization-bindings = on
      }

      remote {
        enabled-transports = ["akka.remote.netty.tcp"]
        netty.tcp {
          hostname = "0.0.0.0"
          port = 2552

          bind-hostname = "0.0.0.0"
          bind-port = 2552

          message-frame-size = 4194304
          send-buffer-size = 1048576000
          receive-buffer-size = 1048576000
          maximum-frame-size = 67108864
        }

        server-socket-worker-pool {
          pool-size-min = 2
          pool-size-factor = 1.0
          pool-size-max = 8
        }

        client-socket-worker-pool {
          pool-size-min = 2
          pool-size-factor = 1.0
          pool-size-max = 8
        }

        default-remote-dispatcher.fork-join-executor {
          parallelism-min = 2
          parallelism-max = 8
        }

        backoff-remote-dispatcher.fork-join-executor {
          parallelism-min = 2
          parallelism-max = 8
        }
      }
    }

    blocking-io-dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 8
      }
      throughput = 1
    }

    statistics-dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 2
      }
      throughput = 1024
    }

    io-engine = "default"

    data-dir = "."

    chunk-length = 4 MiB

    chunk-length-is-power-of-two = true

    small-chunk-length = 64 KiB

    small-chunk-length-is-power-of-two = true

    meta {
      chunk-size = 4 B
      fingerprint = 36 B
      cmd = 4 B
      bagid = 36 B
      offset = 8 B
      size = 8 B
    }

    # ChunkPool size (set to 0 for none)
    chunk-pool-size = 1GiB

    nodes = []

  }

  frontend {
    akka {
      loggers = ["akka.event.slf4j.Slf4jLogger"]
      loglevel = "INFO"
      logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

      actor {
        provider = "akka.remote.RemoteActorRefProvider"

        #allow-java-serialization = off

        serializers {
          wire = "ch.epfl.labos.hailstorm.serialization.WireSerializer"
        }

        serialization-bindings {
          "ch.epfl.labos.hailstorm.common.HailstormMessage" = wire
          "java.io.Serializable" = none
        }

        enable-additional-serialization-bindings = on
      }

      #extensions = [ "akka.cluster.metrics.ClusterMetricsExtension" ]
      #cluster.metrics.enabled=off

      remote {
        enabled-transports = ["akka.remote.netty.tcp"]

        netty.tcp {
          hostname = "0.0.0.0"
          port = 3553

          bind-hostname = "0.0.0.0"
          bind-port = 3553

          message-frame-size = 4194304
          send-buffer-size = 1048576000
          receive-buffer-size = 1048576000
          maximum-frame-size = 67108864
        }

        server-socket-worker-pool {
          pool-size-min = 2
          pool-size-factor = 1.0
          pool-size-max = 8
        }

        client-socket-worker-pool {
          pool-size-min = 2
          pool-size-factor = 1.0
          pool-size-max = 8
        }

        default-remote-dispatcher.fork-join-executor {
          parallelism-min = 2
          parallelism-max = 8
        }

        backoff-remote-dispatcher.fork-join-executor {
          parallelism-min = 2
          parallelism-max = 8
        }
      }
    }

    # ChunkPool size (set to 0 for none)
    chunk-pool-size = 1GiB

    lock-fs-metadata = true

    small-reads-enabled = true

    nodes = []

    # Batch sampling factor k = number of outstanding requests per client
    batching-factor = 10

    # Force batching to be this value (irrespective of the number of servers)
    force-batching-factor = 0

    # Buffer size (in chunks) for a source, i.e. filler
    source-buffer = 16

    # Retry duration for a source, i.e. filler
    source-retry = 30 s

    # Buffer size (in chunks) for a sink, i.e. drainer
    sink-queue = 16

    # Retry duration for a sink, i.e. drainer
    sink-retry = 30 s

    # Retry duration for synchronization barrier
    sync-barrier-retry = 1 s

    # Frequency of scheduler tick (how often to pull for work)
    scheduler-tick = 1 s

    # Compaction offloading
    alpha = 0.5
    theta = 0.2

    # Frontend parallelism
    parallelism = 1

    # IO mode
    # possible values: default/spread (the default), input-local (all input to local disk), output-local (all output to local disk)
    io-mode = "default"
  }

  app {

    master.id = 0

  }

  hdfs {

    username = "Hailstorm"
    hadoop_home = "/usr/local/opt/hailstorm"
    replication_factor = 3

  }

  protocol = "akka.tcp"

  mode = "dev"

  legacy = false

  legacy-heuristic = false

  me = -1
}
